{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twin Delayed Deep Deterministic Policy Gradients (TD3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import site\n",
    "import torch\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "site.addsitedir('../src/')\n",
    "\n",
    "from datetime import date\n",
    "from td3_agent import Agent\n",
    "from collections import deque\n",
    "from model import TD3CriticNetwork, TD3ActorDSNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Results Directory\n",
    "dirs = os.listdir('.')\n",
    "if not any('result' in d for d in dirs):\n",
    "    result_id = 1\n",
    "else:\n",
    "    results = [d for d in dirs if 'result' in d]\n",
    "    result_id = len(results) + 1\n",
    "\n",
    "# Get today's date and add it to the results directory\n",
    "d = date.today()\n",
    "result_dir = 'td3_result_' + str(result_id) + '_{}'.format(\n",
    "    str(d.year) + str(d.month) + str(d.day))\n",
    "os.mkdir(result_dir)\n",
    "print('Created Directory {} to store the results in'.format(result_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "n_episodes = 1e6\n",
    "batch_size = 128\n",
    "\n",
    "seeds = np.load('../seeds/training_seeds.npy')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_learning_rate = 0.001\n",
    "critic_learning_rate = 0.001\n",
    "tau = 0.005\n",
    "layer1_size = 256\n",
    "layer2_size = 256\n",
    "noise = 0.1\n",
    "warmup = 1000\n",
    "update_actor_interval = 2\n",
    "update_target_interval = 2\n",
    "buffer_size = int(2e5)\n",
    "pop_size = 10\n",
    "two_neuron = True\n",
    "pop_coding = False\n",
    "mutually_exclusive = False\n",
    "obs_range = [(-1,1), (-1,1), (-8,8)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_alpha = 0.5\n",
    "actor_beta = 0.5\n",
    "weight_scale = 1\n",
    "actor_threshold = 1\n",
    "actor_sim_time = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "smoothed_scores_all = []\n",
    "\n",
    "for i in range(n_runs):\n",
    "    print(\"Run # {}\".format(i))\n",
    "\n",
    "    seed = int(seeds[i])\n",
    "    \n",
    "    env = gym.make('Pendulum-v1')\n",
    "    \n",
    "    if two_neuron:\n",
    "        input_dims = (env.observation_space.shape[0]*2,)\n",
    "    elif pop_coding:\n",
    "        input_dims = (env.observation_space.shape[0]*pop_size,)\n",
    "    else:\n",
    "        input_dims = env.observation_space.shape\n",
    "    n_actions = env.action_space.shape[0]\n",
    "\n",
    "    actor_architecture = [input_dims[0], layer1_size, layer2_size, n_actions]\n",
    "    critic_architecture = [input_dims[0] + n_actions, layer1_size, layer2_size, n_actions]\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    actor = TD3ActorDSNN(actor_architecture, seed, actor_alpha, actor_beta, weight_scale, 1,\n",
    "                         actor_threshold, actor_sim_time, actor_learning_rate, name='actor_{}'.format(i),\n",
    "                         device=device)\n",
    "    target_actor = TD3ActorDSNN(actor_architecture, seed, actor_alpha, actor_beta, weight_scale, 1,\n",
    "                                actor_threshold, actor_sim_time, actor_learning_rate,\n",
    "                                name='target_actor_{}'.format(i), device=device)\n",
    "    critic_1 = TD3CriticNetwork(critic_learning_rate, input_dims, layer1_size,\n",
    "                                layer2_size, n_actions=n_actions, name='critic_1_{}'.format(i))\n",
    "    critic_2 = TD3CriticNetwork(critic_learning_rate, input_dims, layer1_size,\n",
    "                                layer2_size, n_actions=n_actions, name='critic_2_{}'.format(i))\n",
    "    target_critic_1 = TD3CriticNetwork(critic_learning_rate, input_dims, layer1_size,\n",
    "                                    layer2_size, n_actions=n_actions, name='target_critic_1_{}'.format(i))\n",
    "    target_critic_2 = TD3CriticNetwork(critic_learning_rate, input_dims, layer1_size,\n",
    "                                    layer2_size, n_actions=n_actions, name='target_critic_2_{}'.format(i))\n",
    "\n",
    "    agent = Agent(actor, critic_1, critic_2, target_actor, target_critic_1, target_critic_2,\n",
    "                  input_dims, tau, env, n_episodes, result_dir, n_actions=n_actions, seed=seed,\n",
    "                  noise=noise, update_actor_interval=update_actor_interval, warmup=warmup,\n",
    "                  update_target_interval=update_target_interval, two_neuron=two_neuron,\n",
    "                  buffer_size=buffer_size, spiking=True, spiking_critic=False, normalize=True)\n",
    "\n",
    "    smoothed_scores, reward_history, best_average, best_average_after = agent.train_agent()\n",
    "    smoothed_scores_all.append(smoothed_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_smoothed_scores = [smoothed_scores_all[i] for i in range(n_runs)]\n",
    "mean_smoothed_scores = np.mean(final_smoothed_scores, axis=0)\n",
    "std_smoothed_scores = np.std(final_smoothed_scores, axis=0)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(range(len(final_smoothed_scores[0])), mean_smoothed_scores)\n",
    "plt.fill_between(range(len(final_smoothed_scores[0])),\n",
    "                 np.nanpercentile(final_smoothed_scores, 5, axis=0),\n",
    "                 np.nanpercentile(final_smoothed_scores, 95, axis=0), alpha=0.25)\n",
    "\n",
    "plt.ylim(-1600, 0)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig(result_dir + '/td3_training_snn_pendulum.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
