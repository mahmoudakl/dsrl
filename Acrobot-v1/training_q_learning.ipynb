{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN vs. DSQN for the CartPole Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import site\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "site.addsitedir('../src/')\n",
    "\n",
    "from datetime import date\n",
    "from model import QNetwork, DSNN\n",
    "from dqn_agent import Agent, ReplayBuffer\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment specific parameters\n",
    "env_name = 'Acrobot-v1'\n",
    "n_runs = 5\n",
    "n_evaluations = 100\n",
    "max_steps = 500\n",
    "num_episodes = 500\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Results Directory\n",
    "dirs = os.listdir('.')\n",
    "if not any('result' in d for d in dirs):\n",
    "    result_id = 1\n",
    "else:\n",
    "    results = [d for d in dirs if 'result' in d]\n",
    "    result_id = len(results) + 1\n",
    "\n",
    "# Get today's date and add it to the results directory\n",
    "d = date.today()\n",
    "result_dir = 'dqn_result_' + str(result_id) + '_{}'.format(\n",
    "    str(d.year) + str(d.month) + str(d.day))\n",
    "os.mkdir(result_dir)\n",
    "print('Created Directory {} to store the results in'.format(result_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "discount_factor = 0.999\n",
    "eps_start = 1.0\n",
    "eps_end = 0.05\n",
    "eps_decay = 0.999\n",
    "update_every = 4\n",
    "target_update_frequency = 100\n",
    "learning_rate = 0.001\n",
    "replay_memory_size = 4*10**4\n",
    "tau = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNN Hyperparameters\n",
    "simulation_time = 3\n",
    "alpha = 0.8\n",
    "beta = 0.8\n",
    "threshold = 1.0\n",
    "weight_scale = 1\n",
    "architecture = [6, 256, 256, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = np.load('../seeds/training_seeds.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "smoothed_scores_dqn_all = []\n",
    "dqn_completion_after = []\n",
    "\n",
    "for i_run in range(n_runs):\n",
    "    print(\"Run # {}\".format(i_run))\n",
    "    seed = int(seeds[i_run])\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    policy_net = QNetwork(architecture, seed).to(device)\n",
    "    target_net = QNetwork(architecture, seed).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "    agent = Agent(env_name, policy_net, target_net, architecture, batch_size,\n",
    "                  replay_memory_size, discount_factor, eps_start, eps_end, eps_decay,\n",
    "                  update_every, target_update_frequency, optimizer, learning_rate,\n",
    "                  num_episodes, max_steps, i_run, result_dir, seed, tau)\n",
    "    \n",
    "    smoothed_scores, scores, best_average, best_average_after = agent.train_agent()\n",
    "\n",
    "    np.save(result_dir + '/scores_{}'.format(i_run), scores)\n",
    "    np.save(result_dir + '/smoothed_scores_DQN_{}'.format(i_run), smoothed_scores)\n",
    "\n",
    "    # save smoothed scores in list to plot later\n",
    "    dqn_completion_after.append(best_average_after)\n",
    "    smoothed_scores_dqn_all.append(smoothed_scores)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot scores of individual runs\n",
    "for i in range(len(smoothed_scores_dqn_all)):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(smoothed_scores_dqn_all[i])\n",
    "    plt.ylim(-550, 0)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(result_dir + '/training_dqn_{}.png'.format(i), dpi=1000)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results (mean)\n",
    "best_smoothed_scores_dqn = [smoothed_scores_dqn_all[0],\n",
    "                            smoothed_scores_dqn_all[1],\n",
    "                            smoothed_scores_dqn_all[2],\n",
    "                            smoothed_scores_dqn_all[3],\n",
    "                            smoothed_scores_dqn_all[4]]\n",
    "mean_smoothed_scores_dqn = np.mean(best_smoothed_scores_dqn, axis=0)\n",
    "std_smoothed_scores = np.std(best_smoothed_scores_dqn, axis=0)\n",
    "\n",
    "avg_dqn_completion_after = np.mean([dqn_completion_after[0],\n",
    "                                    dqn_completion_after[1],\n",
    "                                    dqn_completion_after[2],\n",
    "                                    dqn_completion_after[3],\n",
    "                                    dqn_completion_after[4]])\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(range(len(best_smoothed_scores_dqn[0])), mean_smoothed_scores_dqn)\n",
    "plt.fill_between(range(len(best_smoothed_scores_dqn[0])),\n",
    "                 np.nanpercentile(best_smoothed_scores_dqn, 2, axis=0),\n",
    "                 np.nanpercentile(best_smoothed_scores_dqn, 97, axis=0), alpha=0.25)\n",
    "plt.ylim(-550, 0)\n",
    "plt.grid(True)\n",
    "plt.savefig(result_dir + '/DQN_training.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSQN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "smoothed_scores_dsqn_all = []\n",
    "dsqn_completion_after = []\n",
    "\n",
    "for i_run in range(n_runs):\n",
    "    print(\"Run # {}\".format(i_run))\n",
    "    seed = int(seeds[i_run])\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    policy_net = DSNN(architecture, seed, alpha, beta, weight_scale, batch_size, threshold,\n",
    "                      simulation_time, learning_rate)\n",
    "    target_net = DSNN(architecture, seed, alpha, beta, weight_scale, batch_size, threshold,\n",
    "                      simulation_time, learning_rate)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "    agent = Agent(env_name, policy_net, target_net, architecture, batch_size,\n",
    "                  replay_memory_size, discount_factor, eps_start, eps_end, eps_decay,\n",
    "                  update_every, target_update_frequency, optimizer, learning_rate,\n",
    "                  num_episodes, max_steps, i_run, result_dir, seed, tau, spiking=True)\n",
    "\n",
    "    smoothed_scores, scores, best_average, best_average_after = agent.train_agent()\n",
    "\n",
    "    np.save(result_dir + '/scores_{}'.format(i_run), scores)\n",
    "    np.save(result_dir + '/smoothed_scores_DSQN_{}'.format(i_run), smoothed_scores)\n",
    "\n",
    "    # save smoothed scores in list to plot later\n",
    "    smoothed_scores_dsqn_all.append(smoothed_scores)\n",
    "    dsqn_completion_after.append(best_average_after)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_smoothed_scores_dsqn = [smoothed_scores_dsqn_all[0],\n",
    "                             smoothed_scores_dsqn_all[1],\n",
    "                             smoothed_scores_dsqn_all[2],\n",
    "                             smoothed_scores_dsqn_all[3],\n",
    "                             smoothed_scores_dsqn_all[4]]\n",
    "mean_smoothed_scores_dsqn = np.mean(best_smoothed_scores_dsqn, axis=0)\n",
    "\n",
    "avg_dsqn_completion_after = np.mean([dsqn_completion_after[0],\n",
    "                                    dsqn_completion_after[1],\n",
    "                                    dsqn_completion_after[2],\n",
    "                                    dsqn_completion_after[3],\n",
    "                                    dsqn_completion_after[4]])\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(range(len(best_smoothed_scores_dsqn[0])), mean_smoothed_scores_dsqn)\n",
    "plt.fill_between(range(len(best_smoothed_scores_dsqn[0])),\n",
    "                 np.nanpercentile(best_smoothed_scores_dsqn, 2, axis=0),\n",
    "                 np.nanpercentile(best_smoothed_scores_dsqn, 97, axis=0), alpha=0.25)\n",
    "\n",
    "plt.vlines(avg_dsqn_completion_after, 0, 250, 'C0')\n",
    "\n",
    "\n",
    "plt.ylim(-550, 0)\n",
    "plt.grid(True)\n",
    "plt.savefig(result_dir + '/DSQN_training.png', dpi=1000)\n",
    "plt.title('Acrobot-v1 DSQN')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
